{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from requests_html import HTMLSession\n",
    "from urllib.parse import parse_qs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Get available cities\n",
    "# 2. Get available businesses\n",
    "# 3. Get available restaurants in each city\n",
    "# 4. Get urls for each business\n",
    "# 5. Get website links for each business\n",
    "# 6. Get reviews and number of reviews for each\n",
    "# 7. Get phone numbers\n",
    "# 8. Get email_ids\n",
    "# 9. Get addresses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  SAVE ALL THESE IN CSV SHEETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Generating city names and generating urls with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def city_names_extracter(country):\n",
    "    city_names_data = requests.get((\"https://www.yelp.co.uk/locations/countries/{}\").format(country))\n",
    "    city_names_data.status_code\n",
    "\n",
    "    data_city_names = BeautifulSoup(city_names_data.content, 'html.parser')\n",
    "\n",
    "    cities = []\n",
    "    for i in data_city_names.select('li a'):\n",
    "        if i.text!= \"About Yelp\":\n",
    "            cities.append(i.text)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = city_names_extracter('nz')\n",
    "cities_copy = cities\n",
    "len(cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def city_url_generator(cities_list):\n",
    "    city_wise_urls =[]\n",
    "    edited_cities_list = []\n",
    "    for i in cities_list:\n",
    "        if i not in edited_cities_list:\n",
    "                edited_cities_list.append(i)\n",
    "                \n",
    "        if \" \" in i:\n",
    "            cities_list.remove(i)\n",
    "            s = '%2'\n",
    "            edit = i.strip().split()\n",
    "            joined = s.join(edit)\n",
    "            cities_list.append(joined)\n",
    "            \n",
    "    for j in edited_cities_list:\n",
    "        url = \"https://www.yelp.com/search?cflt=restaurants&find_loc={}%2C+New+Zealand\".format(j)\n",
    "        city_wise_urls.append(url)\n",
    "    return city_wise_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_citywise_urls = city_url_generator(cities_copy)\n",
    "all_city_wise_urls_copy = all_citywise_urls\n",
    "len(city_url_generator(cities_copy))\n",
    "all_city_wise_urls_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting number of pages available for each city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pages_for_each_city(cities_urls):\n",
    "    total_pages = []\n",
    "    for t in cities_urls:\n",
    "        h = requests.get(t)\n",
    "        sou = BeautifulSoup(h.content, 'html.parser')\n",
    "        pages_count = sou.find_all('span')[-21].text[-1]\n",
    "        if bool(re.search(r'\\d', pages_count)):\n",
    "            total_pages.append(pages_count)\n",
    "    return total_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bool(re.search(r'\\d', inputString))\n",
    "pages_for_each_city(all_city_wise_urls_copy)\n",
    "total_pages_citywise = pages_for_each_city(all_city_wise_urls_copy)\n",
    "total_pages_citywise_copy = total_pages_citywise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pages_citywise_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def businesses_name_extractor(cities_names_list):\n",
    "    all_names = []\n",
    "    basic_urls_citywise =[]\n",
    "    for n in cities_names_list:\n",
    "        basic_urls_each_city =[]\n",
    "        names_here = []\n",
    "        url = \"https://www.yelp.com/search?cflt=restaurants&find_loc={}%2C+New+Zealand\".format(n)\n",
    "        req_url = requests.get(url)\n",
    "        soup_url = BeautifulSoup(req_url.content, 'html.parser')\n",
    "        here = soup_url.select('h4 span a')\n",
    "        for all_in_here in here:\n",
    "            names_here.append(all_in_here.text)\n",
    "        available_pages = soup_url.find_all('span')[-21].text[-1]\n",
    "        start =10\n",
    "        try:\n",
    "            while start< (10*int(available_pages)):\n",
    "                further_urls =\"https://www.yelp.com/search?cflt=restaurants&find_loc={}&start={}\".format(n, start)\n",
    "                basic_urls_each_city.append(further_urls)\n",
    "                req_further = requests.get(further_urls)\n",
    "                soup_further = BeautifulSoup(req_further.content, 'html.parser')\n",
    "                further_names = soup_further.select('h4 span a')\n",
    "                for each_further in further_names:\n",
    "                    names_here.append(each_further.text)\n",
    "                start = start+10\n",
    "        except:\n",
    "            pass\n",
    "        basic_urls_citywise.append(basic_urls_each_city)\n",
    "        all_names.append(names_here)\n",
    "    return all_names, basic_urls_citywise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants_names = businesses_name_extractor(cities_copy)\n",
    "restaurants_names_copy = restaurants_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_urls_citywise = restaurants_names_copy[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_urls_citywise_copy = basic_urls_citywise\n",
    "basic_urls_citywise_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtering_cities_with_businesses(urls_citywise):\n",
    "    remaining_cities = []\n",
    "    not_available = []\n",
    "    for out in urls_citywise:\n",
    "        try:\n",
    "            rem = out[0].split('find_loc=')[1].split('&')[0]\n",
    "            remaining_cities.append(rem)\n",
    "        except:\n",
    "            not_available.append(out)\n",
    "            continue\n",
    "    return remaining_cities, not_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering_cities_with_businesses(basic_urls_citywise_copy)\n",
    "cities_with_restaurants = filtering_cities_with_businesses(basic_urls_citywise_copy)[0]\n",
    "cities_without_restaurants  = filtering_cities_with_businesses(basic_urls_citywise_copy)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cities_with_restaurants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_without_restaurants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extracting biz elements from href elements and generate urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biz_extractor(given_basic_urls):\n",
    "    extracted = []\n",
    "    for basic in given_basic_urls:\n",
    "        basic_extracted = []\n",
    "        for each_basic in basic:\n",
    "            req = requests.get(each_basic)\n",
    "            print(req.status_code)\n",
    "            status = req.status_code\n",
    "            if status==200:\n",
    "                print('it is 200')\n",
    "            if status==503:\n",
    "                print('503, breaked')\n",
    "                break\n",
    "            try:\n",
    "                soup = BeautifulSoup(req.content, 'html.parser')\n",
    "                selected_a = soup.select('a')\n",
    "                for each_a in selected_a:\n",
    "                    each_hrefs = each_a.attrs.get('href')\n",
    "                    print(each_hrefs)\n",
    "                    string = '/biz/'\n",
    "                    try:\n",
    "                        if string in str(each_hrefs):\n",
    "                            print(each_hrefs)\n",
    "                            try:\n",
    "                                stripped_element = str(each_hrefs).split('?')[0]\n",
    "                                print(stripped_element, 'tried')\n",
    "                            except:\n",
    "                                stripped_element = each_hrefs\n",
    "                                print(stripped_element, 'excepted')\n",
    "                            urls_unique = \"https://www.yelp.com\"+str(stripped_element)\n",
    "                            print(urls_unique)\n",
    "                            if not str(urls_unique) in basic_extracted:\n",
    "                                if '#popup:platform' not in str(urls_unique):\n",
    "                                    print('popup found')\n",
    "                                    basic_extracted.append(urls_unique)\n",
    "                                    print('added as unique')\n",
    "                                        \n",
    "\n",
    "                    except:\n",
    "                        continue\n",
    "            except:\n",
    "                print('503')\n",
    " \n",
    "        print('looped')\n",
    "        extracted.append(basic_extracted)\n",
    "    return extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_biz_links = biz_extractor(basic_urls_citywise_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_biz_links_copy = extracted_biz_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_biz_links_copy\n",
    "# len(extracted_biz_links_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redir_extraction(unique_urls_all):\n",
    "    actual_all_hrefs = []\n",
    "    actual_excepted = []\n",
    "    for outer in range(len(unique_urls_all)):\n",
    "        inner_hrefs = []\n",
    "        inner_excepted = []\n",
    "        print(outer)\n",
    "        for inner in range(len(unique_urls_all[outer])):\n",
    "            actual_url = unique_urls_all[outer][inner]\n",
    "            actual_req = requests.get(actual_url)\n",
    "            print(actual_req.status_code)\n",
    "            actual_soup = BeautifulSoup(actual_req.content, 'html.parser')\n",
    "            a_actual = actual_soup.select('a')\n",
    "            for act in a_actual:\n",
    "                href_actual = act.attrs.get('href')\n",
    "                try:\n",
    "                    if \"/biz_redir?url=\" in href_actual:\n",
    "                        print(href_actual)\n",
    "                        inner_hrefs.append(href_actual)\n",
    "                    else:\n",
    "                        inner_excepted.append(href_actual)\n",
    "\n",
    "                except:\n",
    "                    continue\n",
    "               \n",
    "        actual_excepted.append(inner_excepted)\n",
    "        print(inner_hrefs)\n",
    "        actual_all_hrefs.append(inner_hrefs)\n",
    "    return actual_all_hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redirect_links = redir_extraction(extracted_biz_links_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redirect_links_copy = redirect_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redirect_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(redirect_links_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redirect_links_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [i for i in redirect_links_copy if len(i)!=0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 32-bit",
   "language": "python",
   "name": "python38532bit7da214ebf00a42bea55c6199a7555197"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
